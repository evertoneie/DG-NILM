# -*- coding: utf-8 -*-
"""P1_1 Generating_Segments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tou2-I3CyvbnyYnJxvNH7wvt46MBrw6j
"""

from google.colab import drive
drive.mount('/content/drive')

root_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM"

train_test_data_folder = root_path + '/' + 'pre_processed_data' + '/' + 'train_test_subsets'


"""
Subset Types:

0 - Individual loads without GD
1 - Aggregated loads without GD
2 - Individual loads with GD
3 - Aggregated loads with GD

"""

subset_type = 3

"""
## Creating Train and Test Subsets
```

"""

if subset_type==0:
  file_name = "individual_without_GD"
elif subset_type==1:
  file_name = "aggregated_without_GD"
elif subset_type==2:
  file_name = "individual_with_GD"
elif subset_type==3:
  file_name = "aggregated_with_GD"



dataset_address = root_path + '/' + 'pre_processed_data/' + file_name
print(dataset_address)

from pandas import *
import pandas as pd
import numpy as np
import pickle 
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
#plt.style.use('_mpl-gallery')
# Import full Dataset
i1_df = pd.read_hdf(dataset_address+'/i1_out_entire.hdf5')
i2_df = pd.read_hdf(dataset_address+'/i2_out_entire.hdf5')

# caso não haja inversor, devemos excluir as linhas em que não há cargas acionadas
#i1_df = i1_df.loc[(i1_df["Labels"] != 0)]
#i2_df = i2_df.loc[(i2_df["Labels"] != 0)]

#i1_df = i1_df.loc[(i1_df["Labels"] != 2)]
#i2_df = i2_df.loc[(i2_df["Labels"] != 2)]

#i1_df = i1_df.loc[(i1_df["Labels"] != 3)]
#i2_df = i2_df.loc[(i2_df["Labels"] != 3)]

#i1_df = i1_df.loc[(i1_df["Labels"] != 14)]
#i2_df = i2_df.loc[(i2_df["Labels"] != 14)]



classes_decimal = np.unique(i1_df.Labels)


X1 = np.array([i1_df.loc[:, (i1_df.columns != 'Labels') & (i1_df.columns != 'SW1') &  (i1_df.columns != 'SW2') & (i1_df.columns != 'SW3') & (i1_df.columns != 'SW4')]])
X1 = X1[0,:,:]
#X1avg = np.mean(X1,axis=1) # average value for each example (line)

X2 = np.array([i2_df.loc[:, (i2_df.columns != 'Labels') & (i2_df.columns != 'SW1') &  (i2_df.columns != 'SW2') & (i2_df.columns != 'SW3') & (i2_df.columns != 'SW4')]])
X2 = X2[0,:,:]
#X2avg = np.mean(X2,axis=1) # average value for each example (line)


# Removing the average
#for k in range(X1.shape[0]):
#  X1[k,:] = X1[k,:] - X1avg[k]
#  X2[k,:] = X2[k,:] - X2avg[k]

#X1 = -X1

y = i1_df.loc[:, i1_df.columns == 'Labels']
y = np.array(y)
y = y[:, 0]


y_bin = i1_df[['SW1','SW2','SW3','SW4']]
y_bin = np.array([y_bin])
y_bin = y_bin[0,:,:]


# Normalizing

from sklearn.preprocessing import minmax_scale
# Normalizing between -1 e 1
#sc = StandardScaler()
#sc.fit(X_train)
#X_train = sc.transform(X_train)
#X_test = sc.transform(X_test)

#X1 = minmax_scale(X1, feature_range=(0, 1), axis=0, copy=True)



X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X1,
                                                    y,
                                                    range(len(X1)),
                                                    test_size=0.3,
                                                    random_state=42)

index_train, index_test = np.array([index_train]), np.array([index_test])
index_train, index_test = index_train.flatten(), index_test.flatten()

# Multiple outputs binary labels
y_train_bin = y_bin[index_train,:] 
y_test_bin  = y_bin[index_test,:]

X_train.shape

from matplotlib import pyplot as plt


print('We have ' + str(y_train_bin[:,0].sum())  + ' examples of SW1 for Training')
print('We have ' + str(y_train_bin[:,1].sum())  + ' examples of SW2 for Training')
print('We have ' + str(y_train_bin[:,2].sum())  + ' examples of SW3 for Training')
print('We have ' + str(y_train_bin[:,3].sum())  + ' examples of SW4 for Training')

print('We have ' + str(y_test_bin[:,0].sum())  + ' examples of SW1 for Testing')
print('We have ' + str(y_test_bin[:,1].sum())  + ' examples of SW2 for Testing')
print('We have ' + str(y_test_bin[:,2].sum())  + ' examples of SW3 for Testing')
print('We have ' + str(y_test_bin[:,3].sum())  + ' examples of SW4 for Testing')

print('We have ' + str(y_bin[:,0].sum())  + ' examples of SW1 in the entire dataset')
print('We have ' + str(y_bin[:,1].sum())  + ' examples of SW2 in the entire dataset')
print('We have ' + str(y_bin[:,2].sum())  + ' examples of SW3 in the entire dataset')
print('We have ' + str(y_bin[:,3].sum())  + ' examples of SW4 in the entire dataset')

print('We have ' + str(y_train_bin.shape[0]+y_test_bin.shape[0]) + ' examples')
#print('We have ' + str(y.shape[0]) + ' examples')

plt.hist(y, 16, rwidth=0.9)
plt.show()

train_test_data_folder = root_path + '/' + 'pre_processed_data' + '/' + 'train_test_subsets/' + file_name
import os

try:
  os.makedirs(train_test_data_folder)
  print("File created!")
except:
  print("File already exist! Ok!")


if os.path.exists(train_test_data_folder+'/x_train_subset.hdf5'):
  os.remove(train_test_data_folder+'/x_train_subset.hdf5')

if os.path.exists(train_test_data_folder+'/x_test_subset.hdf5'):
  os.remove(train_test_data_folder+'/x_test_subset.hdf5')

if os.path.exists(train_test_data_folder+'/y_train_subset.hdf5'):
  os.remove(train_test_data_folder+'/y_train_subset.hdf5')

if os.path.exists(train_test_data_folder+'/y_test_subset.hdf5'):
  os.remove(train_test_data_folder+'/y_test_subset.hdf5')

# For the training subset
df_train = pd.DataFrame(data=X_train)
df_train = df_train.assign(i_train=index_train)
df_train.to_hdf(train_test_data_folder+'/x_train_subset.hdf5','data',append=True)        

del df_train
df_train = pd.DataFrame(data=y_train_bin)
df_train = df_train.assign(i_train=index_train)
df_train.to_hdf(train_test_data_folder+'/y_train_subset.hdf5','data',append=True)        

# For the testing subset
df_test = pd.DataFrame(data=X_test)
df_test = df_test.assign(i_test=index_test)
df_test.to_hdf(train_test_data_folder+'/x_test_subset.hdf5','data',append=True)        


del df_test
df_test = pd.DataFrame(data=y_test_bin)
df_test = df_test.assign(i_test=index_test)
df_test.to_hdf(train_test_data_folder+'/y_test_subset.hdf5','data',append=True)

# Ploting an Example

# make data
x1 = X_train[30,:]
x2 = X_test[30,:]
# plot
fig, ax = plt.subplots()

ax.plot(x1, linewidth=2.0)
ax.plot(x2, linewidth=2.0)
ax.set(xlim=(0, 50))
#       ylim=(0, 8), yticks=np.arange(1, 8))

plt.show()


print(y_train[30])



"""# Run from here if you only need to create new segments!"""

train_test_data_folder = root_path + '/' + 'pre_processed_data' + '/' + 'train_test_subsets/' + file_name


from pandas import *
import pandas as pd
import pickle 

# Importando os dados pré-processados
X_train_df = pd.read_hdf(train_test_data_folder+'/x_train_subset.hdf5')
X_test_df = pd.read_hdf(train_test_data_folder+'/x_test_subset.hdf5')
y_train_df = pd.read_hdf(train_test_data_folder+'/y_train_subset.hdf5')
y_test_df = pd.read_hdf(train_test_data_folder+'/y_test_subset.hdf5')

length = 1000

X_train = X_train_df.iloc[:,0:length]
X_test = X_test_df.iloc[:,0:length]


y_train_bin = y_train_df.iloc[:,0:4]
y_test_bin = y_test_df.iloc[:,0:4]

y_train_df.shape

print(y_train_bin.shape)

# Vamos reunir os dados de treino e teste em uma só estrutura para passa-la para frente

import numpy as np

Features = X_train

Features = np.append(Features, X_test, axis=0)


Labels = y_train_bin

Labels = np.append(Labels, y_test_bin, axis=0)



print("A matriz de features possui tamanho " + str(Features.shape) )

print("A matriz de labels binários possui tamanho " + str(Labels.shape) )



time_steps = 100 # Esse é o tamanho de cada janela recortada a partir da amostra importada
n_windows = X_train.shape[1] / time_steps # Quantidade de janelas para cada amostra de entrada
n_windows_total_train = n_windows*X_train.shape[0] 
n_windows_total_test  =  n_windows*X_test.shape[0]
number_of_features = 1 # Isso representa quantas séries temporais nós temos.. como utilizaremos somente uma (corrente agregada), então essa variável vale 1


print(n_windows_total_train)

import os
def remove_if_exists(address):
  if os.path.exists(address):
    os.remove(address)

if subset_type==0:
  file_name = "data_individual_without_GD.mat"
elif subset_type==1:
  file_name = "data_aggregated_without_GD.mat"
elif subset_type==2:
  file_name = "data_individual_with_GD.mat"
elif subset_type==3:
  file_name = "data_aggregated_with_GD.mat"


remove_if_exists(root_path + '/' + file_name)


import scipy.io as sio

# collect arrays in dictionary
savedict = {
    'data' : Features,
    'labels' : Labels,
}

# save to disk

sio.savemat(root_path + '/' + file_name, savedict)
