# -*- coding: utf-8 -*-
"""P1_pre_processing_all_folder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjJQeSqLSF_ghnsaHeNdNQ9F1g70irU1
"""

from google.colab import drive
drive.mount('/content/drive')

#import sys
#sys.modules[__name__].__dict__.clear()

from pandas import *
import pandas as pd
import os
import numpy as np
import csv
from numpy import genfromtxt
from pandas import read_csv
import pickle


# save numpy array as csv file
from numpy import asarray
from numpy import savetxt

labels_out = np.array([])
i1_out = np.array([])
i2_out = np.array([])

#nsamples = 16000

stride = 100 # passo entre uma amostra e outra
ncontinous = 1000 # número de amostras contínuas de mesmo label que devem existir para que consideremos uma amostra de extração de features válida
root_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM"
source_dir_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM/raw_data_without_GD/individual" # pasta das amostras cruas, sem inversor (Source)

print(os.listdir(source_dir_path))

# Remove all files in the new folder

for f in os.listdir(source_dir_path+'/new'):
   os.remove(os.path.join(source_dir_path+'/new', f))

for path in os.listdir(source_dir_path):
    if os.path.isfile(os.path.join(source_dir_path,path)):
        csv_path = os.path.join(source_dir_path,path)
        new_csv_path = os.path.join(source_dir_path,"new",path)

        try:
           # data = genfromtxt(csv_path, delimiter=',')
           # labels = data[:,2]
           # full_sample = data[:,0:1]
           # data = []


           with open(csv_path, "r") as f_old:
               #lines = f_new.readlines()[1:] # all except first
               data = f_old.read().splitlines(True)

           with open(new_csv_path,"w") as f_new:
               f_new.writelines(data[1:])


           del data



           df = pd.read_csv(new_csv_path,sep='\t')
           data = df.values

           labels = data[:,0]
           #full_sample = data[:,0:2]

           data_out = []
           #labels_out = []

           for cont in range(0, labels.size - ncontinous, stride):
               try:
                   arr_labels = labels[cont:cont + ncontinous]  # tests window
                   result = np.all(arr_labels == arr_labels[0])  # checking if all elements from column 5 are the same
                   if result:
                       #data_out = asarray(data[cont:cont + ncontinous,:])
                       #with open('pre_processed_data/data_out_' + str(cont) + '.txt', 'wb') as f:
                        #   pickle.dump(data_out, f)

                       if i1_out.shape[0]==0:
                           i1_out = (data[cont:cont + ncontinous,1])
                           i1_out = i1_out.reshape((1,i1_out.shape[0]))
                       else:
                           i1_out = np.vstack((i1_out,(data[cont:cont + ncontinous,1])))

                       if i2_out.shape[0]==0:
                           i2_out = (data[cont:cont + ncontinous,2])
                           i2_out = i2_out.reshape((1,i2_out.shape[0]))

                       else:
                           i2_out = np.vstack((i2_out,asarray(data[cont:cont + ncontinous,2])))

                       if labels_out.shape[0]==0:
                           labels_out = np.array([arr_labels[0]])
                           #labels_out = labels_out.reshape((1,labels_out.shape[0]))

                       else:
                           #labels_out = np.vstack((labels_out,arr_labels[0]))
                           labels_out = np.append(labels_out,arr_labels[0])


               except:
                   print("Out of bounds")


        except:
            print("Invalid file.")
    else:
        print("There is no file in the folder.")

i1_out_avg = np.mean(i1_out,axis=1) # average value for each example (line)
i2_out_avg = np.mean(i2_out,axis=1) # average value for each example (line)

# Removing the average
for k in range(i1_out.shape[0]):
  i1_out[k,:] = i1_out[k,:] - i1_out_avg[k]
  i1_out[k,:] = i1_out[k,:]*0.036450093058151
  i2_out[k,:] = i2_out[k,:] - i2_out_avg[k]
  i2_out[k,:] = i2_out[k,:]*0.018739120738522

#i1_out = -i1_out # Inversion because the sensor position.

import matplotlib.pyplot as plt

#plt.style.use('_mpl-gallery')

# make data
x1 = i1_out[4000,:]
x2 = i2_out[4000,:]
# plot
fig, ax = plt.subplots()

ax.plot(x1, linewidth=2.0)
ax.plot(x2, linewidth=2.0)
ax.legend(['Aggregated', 'Inverter'])
ax.set(xlim=(0, 50))

#       ylim=(0, 8), yticks=np.arange(1, 8))

plt.show()

if os.path.exists(root_path + '/pre_processed_data/i1_out_entire.hdf5'):
  os.remove(root_path + '/pre_processed_data/i1_out_entire.hdf5')

if os.path.exists(root_path + '/pre_processed_data/i2_out_entire.hdf5'):
  os.remove(root_path + '/pre_processed_data/i2_out_entire.hdf5')

print(root_path + '/pre_processed_data/i1_out_entire.hdf5')

print(labels_out[20:200])

#Saving


# Converting int label to binary label

y_bin = ["{0:04b}".format(i) for i in labels_out]
y_bin = np.array(y_bin)

sw1 = np.zeros([y_bin.shape[0]])
sw2 = np.zeros([y_bin.shape[0]])
sw3 = np.zeros([y_bin.shape[0]])
sw4 = np.zeros([y_bin.shape[0]])

for k in range(y_bin.shape[0]):
    sw1[k] = y_bin[k][0]
    sw2[k] = y_bin[k][1]
    sw3[k] = y_bin[k][2]
    sw4[k] = y_bin[k][3]


# For the first Sensor
df1 = pd.DataFrame(data=i1_out)
df1 = df1.assign(SW1=sw1)
df1 = df1.assign(SW2=sw2)
df1 = df1.assign(SW3=sw3)
df1 = df1.assign(SW4=sw4)
df1 = df1.assign(Labels=labels_out)

#if os.path.exists(root_path + '/i1_out_entire.hdf5'):
#  os.remove(root_path + '/i1_out_entire.hdf5')
#savetxt('pre_processed_data/data_out'+str(cont)+'.csv',data_out, delimiter=',')
df1.to_hdf('/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM/pre_processed_data/i1_out_entire.hdf5','data',append=True)
# add the labels
del df1


# For the second Sensor
df2 = pd.DataFrame(data=i2_out)
df2 = df2.assign(SW1=sw1)
df2 = df2.assign(SW2=sw2)
df2 = df2.assign(SW3=sw3)
df2 = df2.assign(SW4=sw4)
df2 = df2.assign(Labels=labels_out)

#if os.path.exists(root_path + '/i2_out_entire.hdf5'):
#  os.remove(root_path + '/i2_out_entire.hdf5')
#savetxt('pre_processed_data/data_out'+str(cont)+'.csv',data_out, delimiter=',')
df2.to_hdf('/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM/pre_processed_data/i2_out_entire.hdf5','data',append=True)
# add the labels

del df2

print("The End")

