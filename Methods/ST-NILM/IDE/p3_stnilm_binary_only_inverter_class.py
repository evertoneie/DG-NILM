# -*- coding: utf-8 -*-
"""[evertoneie] P3_ST-NILM_GD_NILM_Detect_Only_Inverter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eiXd3uxwXqFu-He6PpDyO4REq7AdwEZJ

## Pre-Definitions and importing
"""

# root_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/Multi_Label_GD_NILM"
# root_path = "/content/drive/MyDrive/Colab/Multi_Label_GD_NILM"

LSTM_TR_path = "/media/everton/Dados_SATA/Artigo_Dataset/Colab/LSTM_TR/LSTM_TR_SRC"
# LSTM_TR_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/LSTM-TR/Multilabel-timeseries-classification-with-LSTM-master"
train_flag = True
# segments_path = "/content/drive/MyDrive/Doutorado/Artigo_Dataset/Colab/LSTM-TR/Multilabel-timeseries-classification-with-LSTM-master/segments"
segments_path = "/media/everton/Dados_SATA/Artigo_Dataset/Colab/LSTM_TR/LSTM_TR_SRC/segments"

J = 4
Q = 8
second_order_flag = True

from sklearn.preprocessing import MaxAbsScaler
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping
from tensorflow.keras.optimizers import Adam
import numpy as np
import os
import pickle
import sys
sys.path.append("/media/everton/Dados_SATA/Downloads/Scattering_Download/Scattering_Novo/src_colab/src")
from DataHandler import DataHandler
from ModelHandler import ModelHandler
from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
from skmultilearn.model_selection import iterative_train_test_split
from sklearn.model_selection import KFold
from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight

configs = {
    "N_GRIDS": 5,
    "SIGNAL_BASE_LENGTH": 512,
    "N_CLASS": 1,
    "USE_NO_LOAD": False,
    "USE_HAND_AUGMENTATION": True,
    "MARGIN_RATIO": 0.15,
    "DATASET_PATH": "drive/MyDrive/Scattering_Novo/dataset_original/Synthetic_Full_iHall.hdf5",
    "TRAIN_SIZE": 0.9,
    "FOLDER_PATH": "/media/everton/Dados_SATA/Artigo_Dataset/Colab/ST-NILM/SC3_Classify_Only_Inverter/trained_models/",
    "FOLDER_DATA_PATH": "/media/everton/Dados_SATA/Artigo_Dataset/Colab/ST-NILM/SC3_Classify_Only_Inverter/trained_models/",
    "N_EPOCHS_TRAINING": 5000,
    "PERCENTUAL": [1],
    "INITIAL_EPOCH": 0,
    "TOTAL_MAX_EPOCHS": 5000,
    "SNRdb": None,  
    "LOSS": 'binary_crossentropy',  # 'bce_weighted_loss', 'focal_loss', "binary_crossentropy", "binary_crossentropy"
}




def freeze(model, task_name='classification'):
    for layer in model.layers:
        if task_name in layer.name:
            layer.trainable = True
        else:
            layer.trainable = False

    for layer in model.layers:
        print(layer.name, layer.trainable)

    return model


def calculating_class_weights(y_true):
    '''
        Source: https://stackoverflow.com/questions/48485870/multi-label-classification-with-class-weights-in-keras
    '''
    from sklearn.utils.class_weight import compute_class_weight
    number_dim = np.shape(y_true)[1]
    weights = np.empty([number_dim, 2])
    for i in range(number_dim):
        weights[i] = compute_class_weight(class_weight='balanced', classes=[0., 1.], y=y_true[:, i])
    return weights


def reduce_dataset(X_all, ydet_all, ytype_all, yclass_all, percentual):
    import numpy as np
    max_index = int(percentual * X_all.shape[0])
    np.random.seed(100)
    index = np.random.randint(max_index, size=(max_index - 1))
    X_all = X_all[index]
    ydet_all = ydet_all[index]
    ytype_all = ytype_all[index]
    yclass_all = yclass_all[index]

    return X_all, ydet_all, ytype_all, yclass_all


ngrids = configs["N_GRIDS"]
signalBaseLength = configs["SIGNAL_BASE_LENGTH"]
trainSize = configs["TRAIN_SIZE"]
folderDataPath = configs["FOLDER_DATA_PATH"]
folderPath = configs["FOLDER_PATH"]

dataHandler = DataHandler(configs)

root_path = "./ST-NILM"
# data_path = "/media/everton/Dados_SATA/Artigo_Dataset/Colab/LSTM_TR/LSTM_TR_SRC/segments"
data_path = "./Multi_Label_GD_NILM"

import sys

sys.path.append(root_path)

import pandas as pd
import numpy as np
import tensorflow as tf
from scipy import stats
from tensorflow.python.ops import rnn, rnn_cell
from sklearn.metrics import roc_auc_score



def read_data(file_path):
    data = pd.read_csv(file_path, header=0)
    return data


def windows(data, window_size):  # 
    start = 0
    while start < len(data["data"]):
        yield int(start), int(start + window_size)  # end = start + window_size
        start += (window_size / 2)


def extract_segments2(data, window_size=30):
    segments = np.empty((0, (window_size), 1))  
    labels = np.empty((0, 4))
    for (start, end) in windows(data,
                                window_size):  
        if (len(data["data"][0, start:end, 0]) == (window_size)):
            signal = data["data"][:, start:end,
                     :]  
            segments = np.vstack([segments, signal])
            
            labels = np.vstack([labels, data["labels"]])  

    return segments, labels 


def extract_segments(data, window_size=30):
    segments = np.empty(
        (0, (window_size + 1))) 
    labels = np.empty((0))
    for (start, end) in windows(data, window_size):
        if (len(data.ix[start:end]) == (window_size + 1)):
            signal = data.ix[start:end]["<FEATURE COLUMN NAME>"]
            segments = np.vstack([segments, signal])
            labels = np.append(labels, stats.mode(data.ix[start:end]["<CLASS COLUMN NAME>"])[0][0])
    return segments, labels  


normalized = 0
sub_selector = [0,2]

# subsets_selector
# 0 - Aggregated
# 1 - Individual
# 2 - All with and without inverter
# 3 - All with inverter
# 4 - All without inverter
# 5 - individual with inverter
# 6 - individual without inverter
# 7 - aggregated with inverter
# 8 - aggregated without inverter


reduced_dataset_flag = False  # Essa flag é para o caso em que fique muito grande o conjunto de treino e teste
load_existing_segments = True

for subsets_selector in sub_selector:

    if subsets_selector == 0:
        complemento = "Detect_Only_Inverter_aggregated_512"
        complemento2 = "_aggregated"
    elif subsets_selector == 1:
        complemento = "Detect_Only_Inverter_individual_512"
        complemento2 = "_individual"
    elif subsets_selector == 2:
        complemento = "Detect_Only_Inverter_all_512"
        complemento2 = "_all"
    elif subsets_selector == 3:
        complemento = "Detect_Only_Inverter_all_with_inverter_512"
        complemento2 = "_all_with_inverter"
    elif subsets_selector == 4:
        complemento = "Detect_Only_Inverter_all_without_inverter_512"
        complemento2 = "_all_without_inverter"
    elif subsets_selector == 5:
        complemento = "Detect_Only_Inverter_individual_with_inverter_512"
        complemento2 = "_individual_with_inverter"
    elif subsets_selector == 6:
        complemento = "Detect_Only_Inverter_individual_without_inverter_512"
        complemento2 = "_individual_without_inverter"
    elif subsets_selector == 7:
        complemento = "Detect_Only_Inverter_aggregated_with_inverter_512"
        complemento2 = "_aggregated_with_inverter"
    elif subsets_selector == 8:
        complemento = "Detect_Only_Inverter_aggregated_without_inverter_512"
        complemento2 = "_aggregated_without_inverter"

    if normalized:
        complemento = "normalized_" + complemento


    def remove_if_exists(address):
        if os.path.exists(address):
            os.remove(address)


    import scipy.io as sio
    from sklearn.model_selection import train_test_split

    """## Generate Segments"""

    if not load_existing_segments:
        win_size = 512  # número de time steps

        # Labels = np.zeros([None,5])
        Labels = np.array([])
        Reshaped_segments = np.array([])

        cont = 0
        if subsets_selector == 1:  # Individual
            options = [0, 2]
        elif subsets_selector == 0:  # Aggregated
            options = [1, 3]
        elif subsets_selector == 2:  # All Subsets
            options = [0, 1, 2, 3]
        elif subsets_selector == 3:  # All with inverter
            options = [2, 3]
        elif subsets_selector == 4:  # All without inverter
            options = [0, 1]
        elif subsets_selector == 5:  # individual with inverter
            options = [2]
        elif subsets_selector == 6:  # individual without inverter
            options = [0]
        elif subsets_selector == 7:  # aggregated with inverter
            options = [3]
        elif subsets_selector == 8:  # aggregated without inverter
            options = [1]

        print("List of Condictions (opctions) that constructs the Segments: ", options)

        for subset_type in options:

            if subset_type == 0:
                file_name = "data_individual_without_GD"
                inverter_flag = 0;
            elif subset_type == 1:
                file_name = "data_aggregated_without_GD"
                inverter_flag = 0;
            elif subset_type == 2:
                file_name = "data_individual_with_GD"
                inverter_flag = 1;
            elif subset_type == 3:
                file_name = "data_aggregated_with_GD"
                inverter_flag = 1;

            if normalized:
                file_name = "normalized_" + file_name

            # load from disk
            data = sio.loadmat(data_path + '/' + file_name)



            data["data"] = data["data"].reshape((data["data"].shape[0], data["data"].shape[1], 1))

            # data = read_data("data.csv")
            # segments,labels = extract_segments(data, win_size)
            segments, labels = extract_segments2(data, win_size)  # 4 labels

            # labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)
            reshaped_segments = segments.reshape([len(segments), (win_size), 1])

            # labels = np.append(labels, np.ones([labels.shape[0], 1]) * inverter_flag,
            #                   axis=1)  # inserindo a quinta coluna (inversor)

            if cont == 0:
                Labels = labels
                Reshaped_segments = reshaped_segments
            else:
                Labels = np.append(Labels, labels, axis=0)  # empilha no eixo das linhas
                Reshaped_segments = np.append(Reshaped_segments, reshaped_segments, axis=0)  # faz o mesmo aqui

            cont = cont + 1
            del labels, segments, reshaped_segments, data


        Labels = Labels.reshape([-1, 4])
        print("Shape of the labels: ", Labels.shape)
        print("Shape of the Segments: ", Reshaped_segments.shape)

        # Train and test Splitting
        if reduced_dataset_flag:

            train_x, val_x, train_y, val_y = train_test_split(Reshaped_segments, Labels,
                                                              stratify=Labels,
                                                              test_size=0.5)  

            train_x, test_x, train_y, test_y = train_test_split(train_x, train_y,
                                                                stratify=train_y,
                                                                test_size=0.2
                                                                )
        else:
            train_x, test_x, train_y, test_y = train_test_split(Reshaped_segments, Labels,
                                                                stratify=Labels,
                                                                test_size=0.2
                                                                )

            # Saving Segments

        if normalized:
            file_name2 = "normalized_Detect_Only_Inverter"
        else:
            file_name2 = "Detect_Only_Inverter"

        if subsets_selector == 1:  # Individual
            file_name2 = file_name2 + "_individual"
        elif subsets_selector == 0:  # Aggregated
            file_name2 = file_name2 + "_aggregated"
        elif subsets_selector == 2:  # All Subsets
            file_name2 = file_name2 + "_all"
        elif subsets_selector == 3:
            file_name2 = file_name2 + "_all_with_inverter"
        elif subsets_selector == 4:
            file_name2 = file_name2 + "_all_without_inverter"
        elif subsets_selector == 5:
            file_name2 = file_name2 + "_individual_with_inverter"
        elif subsets_selector == 6:
            file_name2 = file_name2 + "_individual_without_inverter"
        elif subsets_selector == 7:
            file_name2 = file_name2 + "_aggregated_with_inverter"
        elif subsets_selector == 8:
            file_name2 = file_name2 + "_aggregated_without_inverter"

        if win_size != 100:
            file_name2 = file_name2 + "_" + str(win_size)

        if normalized:
            file_name2 = "normalized_" + file_name2

        import os

       
        segments_path = segments_path + "/" + file_name2

        if not os.path.exists(segments_path):
            # if the demo_folder directory is not present
            # then create it.
            os.makedirs(segments_path)

        # save the segments into the segments_path

        remove_if_exists(segments_path + '/' + "segments.mat")
 

        import scipy.io as sio

        # collect arrays in dictionary
        savedict = {
            'train_x': train_x,
            'train_y': train_y,
            'test_x': test_x,
            'test_y': test_y,
        }
        """
        # collect arrays in dictionary
        savedict_test_x = {
            'data' : Features,
            'labels' : Labels,
        }

        # collect arrays in dictionary
        savedict_train_y = {
            'data' : Features,
            'labels' : Labels,
        }

        # collect arrays in dictionary
        savedict_test_y = {
            'data' : Features,
            'labels' : Labels,
        } """

        # save to disk

        sio.savemat(segments_path + '/' + "segments.mat", savedict)
    

        print("Número total de segmentos: ", (train_x.shape[0] + test_x.shape[0]))
        print(segments_path + '/' + "segments.mat")

    """# Run only if segments exist"""

    if load_existing_segments:

        

        segments_path = segments_path + "/" + complemento

        print(segments_path)


        def remove_if_exists(address):
            if os.path.exists(address):
                os.remove(address)


        win_size = 512
        import os
        import scipy.io as sio

       

        if os.path.exists(segments_path):
            # Load data
            # load segments from disk

            segments_data = sio.loadmat(segments_path + '/' + "segments.mat")

        print(segments_path)

    if not load_existing_segments:
        segments_data = savedict

    train_x = segments_data["train_x"]

    train_y = segments_data["train_y"]
    test_x = segments_data["test_x"]

    test_y = segments_data["test_y"]

    print("Shape of training subset: ", train_x.shape)
    print("Shape of testing subset: ", test_x.shape)
    print("Shape of training labels: ", train_y.shape)
    print("Shape of testing labels: ", test_y.shape)

    print(segments_path)

    modelHandler = ModelHandler(configs)


    X_all = train_x
    yclass = train_y
    yclass = yclass.reshape([yclass.shape[0], 1, yclass.shape[1]])  # reshape to (n_examples,1,n_classes)

    yclass_all = yclass

    # Now we must concatenate yclass n_grid times along axis 1
    for k in range(ngrids - 1):
        yclass_all = np.append(yclass_all, yclass, axis=1)

    print(yclass_all.shape)
    print(yclass.shape)

    yclass_all.shape

    configs["PERCENTUAL"][0]



    num_nan_X = np.count_nonzero(np.isnan(X_all))
    print(num_nan_X)

    num_nan_Y = np.count_nonzero(np.isnan(yclass_all))
    print(num_nan_Y)

    # Find fris order and second order scattering positions

    from kymatio.numpy import Scattering1D as ScatNumpy
    from kymatio.datasets import fetch_fsdd

    scattering = ScatNumpy(J, X_all.shape[1], Q)

    # Sx = scattering(X_all[10])

    meta = scattering.meta()
    order0 = np.where(meta['order'] == 0)
    order1 = np.where(meta['order'] == 1)
    order2 = np.where(meta['order'] == 2)

    print("ZEro order inits on position " + str(order0[0][0]) + " and finishes at " + str(order0[0][-1]))
    print("First order inits on position " + str(order1[0][0]) + " and finishes at " + str(order1[0][-1]))
    print("Second order inits on position " + str(order2[0][0]) + " and finishes at " + str(order2[0][-1]))

    start1 = order1[0][0]
    end1 = order1[0][-1]
    start2 = order2[0][0]
    end2 = order2[0][-1]
    start0 = order0[0][0]
    end0 = order0[0][-1]

    del meta, scattering, order1, order2

    import tensorflow as tf
    from keras.layers import Input, Lambda, GlobalAveragePooling1D, Flatten, MaxPool1D, GlobalMaxPooling1D
    from keras.models import Model
    from kymatio.keras import Scattering1D


    def buildBaseScattering_2order(input_shape, J=J, Q=Q, start1=start1, end1=end1, start2=start2):
        '''
            Source: https://github.com/kymatio/kymatio/blob/master/examples/1d/classif_keras.py
        '''
        log_eps = 1e-3

        input = Input(shape=(input_shape,))
        scattering = Scattering1D(J, Q, max_order=2)
        x = scattering(input)

        print(x)
        print(x.shape)

        # separando os coeficientes de primeira e segunda ordem
        x1 = x[..., 1:end1, :]
        x2 = x[..., end1:, :]

        x1 = Lambda(lambda x1: tf.math.log(tf.abs(x1) + log_eps))(x1)
        x2 = Lambda(lambda x2: tf.math.log(tf.abs(x2) + log_eps))(x2)

        print("Shape of Sx: " + str(x.shape))
        print("Shape of Sx1: " + str(x1.shape))
        print("Shape of Sx2: " + str(x2.shape))

        unmapped_len1 = int(0.15 * (x1.shape[2] / 1.3))
        grid_len1 = int((x1.shape[2] - 2 * unmapped_len1) / 5)

        unmapped_len2 = int(0.15 * (x2.shape[2] / 1.3))
        grid_len2 = int((x2.shape[2] - 2 * unmapped_len2) / 5)

        left1 = Lambda(lambda x1: x1[..., :, : unmapped_len1], name='left1')(x1)
        center1 = Lambda(lambda x1: x1[..., :, unmapped_len1: x1.shape[2] - unmapped_len1], name='center1')(x1)
        right1 = Lambda(lambda x1: x1[..., :, x1.shape[2] - unmapped_len1:], name='right1')(x1)

        left2 = Lambda(lambda x2: x2[..., :, : unmapped_len2], name='left2')(x2)
        center2 = Lambda(lambda x2: x2[..., :, unmapped_len2: x2.shape[2] - unmapped_len2], name='center2')(x2)
        right2 = Lambda(lambda x2: x2[..., :, x2.shape[2] - unmapped_len2:], name='right2')(x2)

        g11 = Lambda(lambda x1: x1[..., :, :grid_len1], name='g11')(center1)
        g21 = Lambda(lambda x1: x1[..., :, grid_len1:2 * grid_len1], name='g21')(center1)
        g31 = Lambda(lambda x1: x1[..., :, 2 * grid_len1:3 * grid_len1], name='g31')(center1)
        g41 = Lambda(lambda x1: x1[..., :, 3 * grid_len1:4 * grid_len1], name='g41')(center1)
        g51 = Lambda(lambda x1: x1[..., :, 4 * grid_len1:], name='g51')(center1)

        leftav1 = tf.keras.backend.mean(left1, axis=2)
        g1av1 = tf.keras.backend.mean(g11, axis=2)
        g2av1 = tf.keras.backend.mean(g21, axis=2)
        g3av1 = tf.keras.backend.mean(g31, axis=2)
        g4av1 = tf.keras.backend.mean(g41, axis=2)
        g5av1 = tf.keras.backend.mean(g51, axis=2)
        rightav1 = tf.keras.backend.mean(right1, axis=2)

        g12 = Lambda(lambda x2: x2[..., :, :grid_len2], name='g12')(center2)
        g22 = Lambda(lambda x2: x2[..., :, grid_len2:2 * grid_len2], name='g22')(center2)
        g32 = Lambda(lambda x2: x2[..., :, 2 * grid_len2:3 * grid_len2], name='g32')(center2)
        g42 = Lambda(lambda x2: x2[..., :, 3 * grid_len2:4 * grid_len2], name='g42')(center2)
        g52 = Lambda(lambda x2: x2[..., :, 4 * grid_len2:], name='g52')(center2)

        leftav2 = tf.keras.backend.mean(left2, axis=2)
        g1av2 = tf.keras.backend.mean(g12, axis=2)
        g2av2 = tf.keras.backend.mean(g22, axis=2)
        g3av2 = tf.keras.backend.mean(g32, axis=2)
        g4av2 = tf.keras.backend.mean(g42, axis=2)
        g5av2 = tf.keras.backend.mean(g52, axis=2)
        rightav2 = tf.keras.backend.mean(right2, axis=2)



        x_class = tf.concat(
            [leftav1, g1av1, g2av1, g3av1, g4av1, g5av1, rightav1, leftav2, g1av2, g2av2, g3av2, g4av2, g5av2,
             rightav2],
            axis=1)

        ##x_type = Flatten()(x_type)
        x_class = Flatten()(x_class)

        # ================================================================

        # x = GlobalAveragePooling1D()(x)

        ##model = Model(inputs=input, outputs=[x_type, x_class])
        model = Model(inputs=input, outputs=[x_class])

        return model


    def buildBaseScattering(input_shape, J, Q):
        '''
            Source: https://github.com/kymatio/kymatio/blob/master/examples/1d/classif_keras.py
        '''
        log_eps = 10e-6

        input = Input(shape=(input_shape,))

        x = Scattering1D(J, Q, max_order=1)(
            input)  # Changed J from 8 to 10 -> Results in a flatten with 544 parameters (the original with convolutions has 520)
        x = Lambda(lambda x: tf.math.log(tf.abs(x) + log_eps))(x)  

        unmapped_len = int(0.15 * (x.shape[2] / 1.3))
        grid_len = int((x.shape[2] - 2 * unmapped_len) / 5)

        print(f"X: {x.shape[2]}, Unmapped: {unmapped_len}, Grid: {grid_len}")

        left = Lambda(lambda x: x[..., :, : unmapped_len], name='left')(x)
        center = Lambda(lambda x: x[..., :, unmapped_len: x.shape[2] - unmapped_len], name='center')(x)
        right = Lambda(lambda x: x[..., :, x.shape[2] - unmapped_len:], name='right')(x)

        g1 = Lambda(lambda x: x[..., :, :grid_len], name='g1')(center)
        g2 = Lambda(lambda x: x[..., :, grid_len:2 * grid_len], name='g2')(center)
        g3 = Lambda(lambda x: x[..., :, 2 * grid_len:3 * grid_len], name='g3')(center)
        g4 = Lambda(lambda x: x[..., :, 3 * grid_len:4 * grid_len], name='g4')(center)
        g5 = Lambda(lambda x: x[..., :, 4 * grid_len:], name='g5')(center)

        leftav = tf.keras.backend.max(left, axis=2)
        g1av = tf.keras.backend.max(g1, axis=2)
        g2av = tf.keras.backend.max(g2, axis=2)
        g3av = tf.keras.backend.max(g3, axis=2)
        g4av = tf.keras.backend.max(g4, axis=2)
        g5av = tf.keras.backend.max(g5, axis=2)
        rightav = tf.keras.backend.max(right, axis=2)



        x_class = tf.concat([leftav, g1av, g2av, g3av, g4av, g5av, rightav], axis=1)

      
        x_class = Flatten()(x_class)

        
        model = Model(inputs=input, outputs=[x_class])

        return model

    if second_order_flag:
        scattering_extract = buildBaseScattering_2order(X_all.shape[1], J=J, Q=Q)
    else:
        scattering_extract = buildBaseScattering(X_all.shape[1], J=J, Q=Q)

    scattering_extract.summary()

    tf.keras.backend.clear_session()

    # Training setup starts here

    X_all.shape

    source_models_folder = configs['FOLDER_PATH']

    if not os.path.exists(source_models_folder):

        os.makedirs(source_models_folder)

    complemento2 = complemento2 + '_' + configs["LOSS"]
    complemento3 = ""

    scattering_extract.save(source_models_folder + 'ST_NILM_' + complemento + '_J' + str(J) + '_Q' + str(Q) + complemento3 + '.h5')

    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import StratifiedKFold

    fold = 0

    mskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    strat_classes = np.max(yclass_all, axis=1)
    print(strat_classes.shape)

    for train_index, validation_index in mskf.split(X_all, strat_classes):
        fold += 1

        print(f"---------- FOLD {fold} -------------")

        scaler = MaxAbsScaler()
        # scaler = StandardScaler()



        scaler.fit(np.squeeze(X_all[train_index], axis=2))
        x_train = np.expand_dims(scaler.transform(np.squeeze(X_all[train_index], axis=2)), axis=2)
        x_validation = np.expand_dims(scaler.transform(np.squeeze(X_all[validation_index], axis=2)), axis=2)



        x_train_class = scattering_extract.predict(x_train)
        x_validation_class = scattering_extract.predict(x_validation)

        # Replacindo infinite values to zero
        x_train_class = np.nan_to_num(x_train_class, nan=0.0, posinf=0.0, neginf=0.0)
        x_validation_class = np.nan_to_num(x_validation_class, nan=0.0, posinf=0.0, neginf=0.0)





        # Normalizing
        nan_x_train = np.count_nonzero(np.isinf(x_train_class))
        nan_x_validation = np.count_nonzero(np.isinf(x_validation_class))



        # transformer = MaxAbsScaler().fit(x_train_class)
        transformer = StandardScaler().fit(x_train_class)
        x_train_class = transformer.transform(x_train_class)

        # transformer = MaxAbsScaler().fit(x_validation_class)
        transformer = StandardScaler().fit(x_validation_class)
        x_validation_class = transformer.transform(x_validation_class)

        print("Each grid has size " + str(x_train[1].shape))

        y_train, y_validation = {}, {}

        y_train["classification"] = yclass_all[train_index]
        y_validation["classification"] = yclass_all[validation_index]

        yclass_weights = calculating_class_weights(np.max(y_train["classification"], axis=1))

        print(yclass_weights)

        if not os.path.exists(
                source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/' + str(
                        fold)):
            # if the demo_folder directory is not present
            # then create it.
            os.makedirs(
                source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/' + str(fold))

        if not os.path.exists(
                source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/logs'):
            os.makedirs(source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/logs')

        np.save(source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/' + str(
            fold) + "/train_index.npy", train_index)
        np.save(source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/' + str(
            fold) + "/validation_index.npy", validation_index)

        tensorboard_callback = TensorBoard(
            log_dir=source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/logs')

        if configs["INITIAL_EPOCH"] > 0:
            model = ModelHandler.loadModel(folderPath + 'model_{0}.h5'.format(configs["INITIAL_EPOCH"]))
        else:
            model = modelHandler.buildScatteringOutput5(input_shape=x_train_class.shape[1])
            #model = modelHandler.buildScatteringOutputBinary(input_shape=x_train_class.shape[1])

        model.summary()

        fileEpoch = configs["INITIAL_EPOCH"]
        while fileEpoch < configs["TOTAL_MAX_EPOCHS"]:
            fileEpoch += configs["N_EPOCHS_TRAINING"]

            if not os.path.isfile(folderPath + 'Detect_Multiclass_Inverter_all_512.h5'):
                # for subtask in ['type', 'classification']:
                for subtask in ['classification']:
                    print(f"FOLD {fold}: Training {subtask}")

                    # freeze(model, task_name=subtask)

                    model.compile(optimizer=Adam(), \
                                  # loss = ["categorical_crossentropy", "binary_crossentropy"], \
                                  loss = ["binary_crossentropy"], \
                                  ##loss = [the_loss], \
                                  #loss=[configs["LOSS"]], \
                                  # loss=["multi_label_focal_loss"], \
                                  metrics=[['binary_accuracy']])
                    # metrics=[['categorical_accuracy'], ['binary_accuracy']])

                    early_stopping_callback = EarlyStopping(monitor="val_binary_accuracy", patience=50, verbose=True,
                                                            restore_best_weights=True)

                    # hist_opt = model.fit(x=[x_train_type, x_train_class], y=[y_train["type"], y_train["classification"]], \
                    #                    validation_data=([x_validation_type, x_validation_class], [y_validation["type"], y_validation["classification"]]), \
                    #                    epochs=configs["N_EPOCHS_TRAINING"], verbose=2, callbacks=[early_stopping_callback, tensorboard_callback], batch_size=32)
                    hist_opt = model.fit(x=[x_train_class], y=[y_train["classification"]], \
                                         validation_data=([x_validation_class], [y_validation["classification"]]), \
                                         epochs=configs["N_EPOCHS_TRAINING"], verbose=2,
                                         callbacks=[early_stopping_callback, tensorboard_callback], batch_size=32)

                if os.path.exists(source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(
                        Q) + complemento2 + '/' + str(fold) + '/' + complemento + '.h5'):
                    os.remove(source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(
                        Q) + complemento2 + '/' + str(fold) + '/' + complemento + '.h5')
                    print('The model exists! We replaced it!')

                # Then save the new trained model
                model.save(
                    source_models_folder + 'ST_NILM' + '/' + '_J' + str(J) + '_Q' + str(Q) + complemento2 + '/' + str(
                        fold) + '/' + complemento + '.h5')

        del model, y_validation, y_train, x_validation, x_train, x_train_class, x_validation_class

